import numpy as np
from numpy import loadtxt
import xgboost
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier  
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
from hyperopt import hp
import pandas as pd

dataset = pd.read_csv(r'C:\Users\Rohit\Desktop\train.csv')
dataset.drop('Loan_ID', axis=1, inplace=True)
dataset.head(5)
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
cat = dataset.select_dtypes(include='O').keys()
cat = list(cat)
for i in cat:
  dataset[i] = le.fit_transform(dataset[i])
for i in dataset.columns:
  dataset[i].fillna(int(dataset[i].mean()), inplace=True)
dataset.to_csv('file2.csv', header=False, index=False)

dataset = loadtxt(r'C:\Users\Rohit\Desktop\file2.csv', delimiter=",")
# split data into X and y
X = dataset[:,0:11]
Y = dataset[:,11]
# CV model
kfold = KFold(n_splits=10)

kfoldn = KFold(n_splits=400)

#random_forest
rfc=RandomForestClassifier(random_state=42)
param_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False], 'criterion' :['gini', 'entropy']}
clf3 = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)
clf3.fit(X, Y)
print(clf3.best_params_)
print(clf3.score(X, Y))

#mlp_classifier
parameters = {'solver': ['lbfgs'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}
clf4 = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)
clf4.fit(X, Y)
print(clf4.score(X, Y))
print(clf4.best_params_)